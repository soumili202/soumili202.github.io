Below is a clean, intuitive, and production-friendly explanation of every mathematical formula used in your topic-shift detector prompt â€” why it's used, how it behaves, and what to tune.

This will help you explain it confidently to your leads, justify design choices, and refine thresholds scientifically.


---

ğŸ”µ 1. Concept Set Extraction (V(H), V(Mâ‚™))

We convert text into concept sets:

V(H),\;V(Mâ‚™)

These represent the core semantic â€œanchorsâ€ of the history and new message (keywords, entities, topics).

Why use this?

It transforms unstructured natural language into structured sets.

Reduces noise from filler words (â€œtheâ€, â€œandâ€, â€œpleaseâ€).

Needed for computing lexical overlap.


Interpretation

If:

V(H) = {jwt, auth, token, fastapi}

V(Mâ‚™) = {trekking, bangalore}


Then the two sets have no intersection â†’ likely new topic.


---

ğŸ”µ 2. Lexical Overlap Formula

O = \frac{|V(H) âˆ© V(Mâ‚™)|}{|V(H) âˆª V(Mâ‚™)|}

What does this measure?

Pure word-level similarity between history and new message.

If two messages share many key terms â†’ topics aligned

If they share none â†’ likely different


Properties



0 means no shared words

1 means identical concept sets


Why this matters

Even without embeddings, lexical overlap gives a crude but fast signal.

Example:

Same topic:

H: "How to fix JWT expiration?"
Mâ‚™: "How to refresh JWT token?"

 or more.

Different topic:

H: "JWT tokens in FastAPI"
Mâ‚™: "Best trekking spots?"





---

ğŸ”µ 3. Semantic Similarity Estimate

S_{sem} âˆˆ [0,1]

Here the LLM provides an estimation of meaning-level similarity.

Why needed?

Lexical overlap alone fails when wording differs:

â€œHow do I secure my API?â€

â€œHow do I protect endpoints?â€


Different vocabulary â†’ similar meaning.

Interpretation

1.0 â†’ identical intent/topic

0.0 â†’ completely unrelated

~0.5 â†’ somewhat related but drifting


Why LLM is good at this

LLMs are strong semantic reasoners; they can infer relationships even without overlapping words.


---

ğŸ”µ 4. Composite Similarity Score

S = Î±Â·O + (1âˆ’Î±)Â·S_{sem}

with

Î± = 0.3

Meaning

We blend two signals:

O â†’ surface-level similarity

Sâ‚›â‚‘â‚˜ â†’ deeper meaning similarity


Why Î± = 0.3?

Semantic meaning is more important than exact word matching.

Many follow-up questions use different surface vocabulary.

Empirically, semantic similarity gives better stability.


So:

30% weight to lexical similarity

70% weight to semantic similarity


Interpretation

If user wording changes but meaning stays â†’ S stays high

If meaning changes â†’ both O and S_sem drop â†’ S becomes low



---

ğŸ”µ 5. Decision Thresholds

Lower Threshold (Topic Shift)

S < Ï„_{low} = 0.55

Below 0.55 â†’ meaning diverged â†’ treat as new topic.

Upper Threshold (Same Topic)

S â‰¥ Ï„_{high} = 0.70

Above 0.70 â†’ strong continuity â†’ retain history.

Middle Zone (Borderline)

If:

0.55 â‰¤ S < 0.70

â†’ ambiguous â€” treat carefully.

Why 0.55 and 0.70?

These values were chosen because:

<0.5 â†’ generally unrelated

> 0.7 â†’ strongly related



0.55â€“0.70 â†’ unclear cases (e.g., shifting subtopics)


This gives a hysteresis-like effect (two boundaries to avoid flicker).


---

ğŸ”µ 6. Follow-Up Detection Rules

Not formula-based but logic-based:
If message contains follow-up phrases:

â€œmore about thisâ€

â€œcontinue thatâ€

â€œshorten itâ€

â€œexplain this furtherâ€

â€œsummarize itâ€

â€œrewrite itâ€

â€œimprove thisâ€


or pronouns:

â€œitâ€, â€œthisâ€, â€œthatâ€


and the referent exists in history â†’ force Same Topic (Follow-up).

Why override the formulas?

Because short follow-ups like:

â€œExplain thatâ€

â€œFix itâ€

â€œContinueâ€

â€œShorten itâ€


would produce:

O â‰ˆ 0, \quad S_{sem} â‰ˆ 0

â€¦giving a false topic shift.

Pronoun-based follow-ups MUST override mathematical scoring.


---

ğŸ”µ 7. Why This Hybrid Approach Works

This combined design gives:

High precision (no unnecessary resets)

Semantic similarity + follow-up detection ensures we donâ€™t drop context prematurely.

High recall (detects real topic shifts)

When domain or intent changes drastically, both O and S_sem drop â†’ S â†’ low.

Deterministic behavior

Because the LLM follows fixed formulas and rules.

Avoids embedding compute

All reasoning is done inside the model, which is valuable when:

Embedding cost is high

No embedding model available

System wants to keep everything inside one LLM call



---

ğŸ¯ Summary of Each Formula

Formula	Meaning	Why Needed

	Concept sets	Structural representation
î€O = \frac{	âˆ©	}{
	Semantic similarity	Meaning-level understanding
	Composite similarity	Balanced, robust scoring
Thresholds 0.55, 0.70	Decision rules	Avoid flicker, classify clearly



---

If you want, I can now create:

ğŸ‘‰ A visual diagram explaining all formulas
ğŸ‘‰ A PDF document summarizing the math + prompt
ğŸ‘‰ A FastAPI code module implementing this detector

Just tell me which one you need.


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TOPIC SHIFT DETECTOR                        â”‚
â”‚               (Mathematical + Follow-Up Rule Based)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     NEW MESSAGE (Mâ‚™)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ 1. FOLLOW-UP PHRASE DETECTION    â”‚
                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                  â”‚ Check if Mâ‚™ contains:            â”‚
                  â”‚  - â€œthisâ€, â€œthatâ€, â€œitâ€          â”‚
                  â”‚  - â€œcontinueâ€, â€œexplain moreâ€    â”‚
                  â”‚  - â€œshorten itâ€, â€œsummarize itâ€  â”‚
                  â”‚  - â€œgo deeperâ€, â€œrewrite thisâ€   â”‚
                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                  â”‚ If YES â†’ DIRECT CLASSIFICATION:  â”‚
                  â”‚     DECISION = Same Topic        â”‚
                  â”‚     DECISION_TYPE = Follow-up    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚(No follow-up)
                                  â–¼


    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 2. EXTRACT CONCEPT SETS FROM HISTORY & Mâ‚™     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Identify keywords/entities â†’ V(H), V(Mâ‚™)      â”‚
    â”‚ Example:                                      â”‚
    â”‚   H: {jwt, token, fastapi}                    â”‚
    â”‚   Mâ‚™: {trekking, bangalore}                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 3. LEXICAL OVERLAP (O)                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                |V(H) âˆ© V(Mâ‚™)|                  â”‚
    â”‚   O = -------------------------------          â”‚
    â”‚                |V(H) âˆª V(Mâ‚™)|                  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Measures: shared words / total unique words   â”‚
    â”‚ Range: 0 (none shared) to 1 (identical sets)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 4. SEMANTIC SIMILARITY (S_sem)                 â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ LLM judges meaning-level closeness             â”‚
    â”‚ S_sem âˆˆ [0,1]                                  â”‚
    â”‚ 0 = unrelated topics                           â”‚
    â”‚ 1 = identical meaning                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 5. COMPOSITE SCORE (S)                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   S = Î±Â·O + (1âˆ’Î±)Â·S_sem                        â”‚
    â”‚   Î± = 0.3                                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Assigns 30% weight to lexical overlap          â”‚
    â”‚ Assigns 70% weight to semantic similarity      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ 6. THRESHOLD DECISION        â”‚
               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
               â”‚ If S < 0.55  â†’ Topic Shift   â”‚
               â”‚ If S â‰¥ 0.70 â†’ Same Topic     â”‚
               â”‚ Else         â†’ Borderline     â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FINAL CLASSIFICATION                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OUTPUT: JSON { similarity_score, decision, summaries, etc. } â”‚
â”‚                                                              â”‚
â”‚ Possible decisions:                                          â”‚
â”‚   â€¢ Same Topic (Follow-up)  â† from follow-up detection step  â”‚
â”‚   â€¢ Same Topic                                                â”‚
â”‚   â€¢ Topic Shift                                               â”‚
â”‚   â€¢ Borderline                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
